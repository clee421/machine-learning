# Chat GPT

## Youtube: Let's build GPT: from scratch, in code, spelled out

[source][1]

## Text Tokenizer

Google uses [sentence piece][2] for unsupervised text tokenizer for neural network-based text
generation.

OpenAI uses [tik token][3] for fast byte pair encoding tokenizer.

## BigramLanguage Model

> In the Bigram Language Model, we find bigrams, which are two words coming together in the
> corpus(the entire collection of words/sentences).

[source][4]

## Public Sources

- [Attention is all you need][6]
- [Language Models are Few-Shot Learners][7]
- [How ChatGPT is built][8]

Stanford has a paper on [N-gram Language Models][5]

[1]: https://www.youtube.com/watch?v=kCc8FmEb1nY
[2]: https://github.com/google/sentencepiece
[3]: https://github.com/openai/tiktoken
[4]: https://www.educative.io/answers/what-is-a-bigram-language-model
[5]: https://web.stanford.edu/~jurafsky/slp3/3.pdf
[6]: https://arxiv.org/pdf/1706.03762.pdf
[7]: https://arxiv.org/pdf/2005.14165.pdf
[8]: https://openai.com/blog/chatgpt